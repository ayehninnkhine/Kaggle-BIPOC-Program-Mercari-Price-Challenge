{"cells":[{"cell_type":"code","execution_count":0,"outputs":[],"metadata":{"collapsed":false,"_kg_hide-input":false},"source":"# %% [code]\n# Based on Bojan -> https://www.kaggle.com/tunguz/more-effective-ridge-lgbm-script-lb-0-44944\n# and Nishant -> https://www.kaggle.com/nishkgp/more-improved-ridge-2-lgbm\n\nimport gc\nimport time\nimport numpy as np\nimport pandas as pd\nfrom scipy.sparse import csr_matrix, hstack\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.preprocessing import LabelEncoder, Normalizer, LabelBinarizer\nfrom sklearn.model_selection import train_test_split\nimport lightgbm as lgb\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Input, Dropout, Dense, concatenate, GRU, Embedding, Flatten, Activation\nfrom keras.optimizers import Adam\nfrom keras.models import Model\nfrom keras import backend as K\n\nimport sys\n\n###Add https://www.kaggle.com/anttip/wordbatch to your kernel Data Sources, \n###until Kaggle admins fix the wordbatch pip package installation\n###sys.path.insert(0, '../input/wordbatch/wordbatch/')\nimport wordbatch\n\nfrom wordbatch.extractors import WordBag, WordHash\nfrom wordbatch.models import FTRL, FM_FTRL\n\nfrom nltk.corpus import stopwords\nimport re\n\nNUM_BRANDS = 4500\nNUM_CATEGORIES = 1200\n\ndevelop = False\n# develop= True\n\ndef rmsle(y, y0):\n    assert len(y) == len(y0)\n    return np.sqrt(np.mean(np.power(np.log1p(y) - np.log1p(y0), 2)))\n\n\ndef split_cat(text):\n    try:\n        return text.split(\"/\")\n    except:\n        return (\"No Label\", \"No Label\", \"No Label\")\n\n\ndef handle_missing_inplace(dataset):\n    dataset['general_cat'].fillna(value='missing', inplace=True)\n    dataset['subcat_1'].fillna(value='missing', inplace=True)\n    dataset['subcat_2'].fillna(value='missing', inplace=True)\n    dataset['brand_name'].fillna(value='missing', inplace=True)\n    dataset['item_description'].fillna(value='missing', inplace=True)\n\n\ndef cutting(dataset):\n    pop_brand = dataset['brand_name'].value_counts().loc[lambda x: x.index != 'missing'].index[:NUM_BRANDS]\n    dataset.loc[~dataset['brand_name'].isin(pop_brand), 'brand_name'] = 'missing'\n    pop_category1 = dataset['general_cat'].value_counts().loc[lambda x: x.index != 'missing'].index[:NUM_CATEGORIES]\n    pop_category2 = dataset['subcat_1'].value_counts().loc[lambda x: x.index != 'missing'].index[:NUM_CATEGORIES]\n    pop_category3 = dataset['subcat_2'].value_counts().loc[lambda x: x.index != 'missing'].index[:NUM_CATEGORIES]\n    dataset.loc[~dataset['general_cat'].isin(pop_category1), 'general_cat'] = 'missing'\n    dataset.loc[~dataset['subcat_1'].isin(pop_category2), 'subcat_1'] = 'missing'\n    dataset.loc[~dataset['subcat_2'].isin(pop_category3), 'subcat_2'] = 'missing'\n\n\ndef to_categorical(dataset):\n    dataset['general_cat'] = dataset['general_cat'].astype('category')\n    dataset['subcat_1'] = dataset['subcat_1'].astype('category')\n    dataset['subcat_2'] = dataset['subcat_2'].astype('category')\n    dataset['item_condition_id'] = dataset['item_condition_id'].astype('category')\n\n\n# Define helpers for text normalization\nstopwords = {x: 1 for x in stopwords.words('english')}\nnon_alphanums = re.compile(u'[^A-Za-z0-9]+')\n\n# get name and description lengths\ndef wordCount(text):\n    try:\n        if text == 'No description yet':\n            return 0\n        else:\n            text = text.lower()\n            words = [w for w in text.split(\" \")]\n            return len(words)\n    except: \n        return 0\n\n\ndef normalize_text(text):\n    return u\" \".join(\n        [x for x in [y for y in non_alphanums.sub(' ', text).lower().strip().split(\" \")] \\\n         if len(x) > 1 and x not in stopwords])\n\ndef normalize_dataset_text(dataset):\n    dataset['item_description'] = dataset['item_description'].apply(lambda x: normalize_text(x))\n    dataset['brand_name'] = dataset['brand_name'].apply(lambda x: normalize_text(x))\n    \ndef text_length_feature(dataset, desc_normalizer, train = True):\n    if train:\n        dataset['desc_len'] = dataset['item_description'].apply(lambda x: wordCount(x))\n        dataset['name_len'] = dataset['name'].apply(lambda x: wordCount(x))\n        dataset[['desc_len', 'name_len']] = desc_normalizer.fit_transform(dataset[['desc_len', 'name_len']])\n    else:\n        dataset['desc_len'] = dataset['item_description'].apply(lambda x: wordCount(x))\n        dataset['name_len'] = dataset['name'].apply(lambda x: wordCount(x))\n        dataset[['desc_len', 'name_len']] = desc_normalizer.transform(dataset[['desc_len', 'name_len']])\n\ndef main():\n    start_time = time.time()\n    from time import gmtime, strftime\n    print(strftime(\"%Y-%m-%d %H:%M:%S\", gmtime()))\n\n    # if 1 == 1:\n    # train = pd.read_table('../input/mercari-price-suggestion-challenge/train.tsv', engine='c')\n    # test = pd.read_table('../input/mercari-price-suggestion-challenge/test.tsv', engine='c')\n\n    train = pd.read_table('../input/mercari/train.tsv', engine='c')\n    test = pd.read_table(\"../input/mercari-price-suggestion-challenge/test_stg2.tsv.zip\" , sep='\\t')\n    # test = pd.read_table('../input/mercari/test.tsv', engine='c')\n\n    print('[{}] Finished to load data'.format(time.time() - start_time))\n    print('Train shape: ', train.shape)\n    print('Test shape: ', test.shape)\n    nrow_test = train.shape[0]  # -dftt.shape[0]\n    dftt = train[(train.price < 1.0)]\n    train = train.drop(train[(train.price < 1.0)].index)\n    del dftt['price']\n    nrow_train = train.shape[0]\n    # print(nrow_train, nrow_test)\n    y = np.log1p(train[\"price\"])\n    merge: pd.DataFrame = pd.concat([train, dftt, test])\n    submission: pd.DataFrame = test[['test_id']]\n\n    del train\n    del test\n    gc.collect()\n\n    merge['general_cat'], merge['subcat_1'], merge['subcat_2'] = \\\n        zip(*merge['category_name'].apply(lambda x: split_cat(x)))\n    merge.drop('category_name', axis=1, inplace=True)\n    print('[{}] Split categories completed.'.format(time.time() - start_time))\n\n    handle_missing_inplace(merge)\n    print('[{}] Handle missing completed.'.format(time.time() - start_time))\n\n    cutting(merge)\n    print('[{}] Cut completed.'.format(time.time() - start_time))\n\n    to_categorical(merge)\n    print('[{}] Convert categorical completed'.format(time.time() - start_time))\n    wb = wordbatch.WordBatch(normalize_text, extractor=(WordBag, {\"hash_ngrams\": 2, \"hash_ngrams_weights\": [1.5, 1.0],\n                                                                  \"hash_size\": 2 ** 29, \"norm\": None, \"tf\": 'binary',\n                                                                  \"idf\": None,\n                                                                  }), procs=8)\n    wb.dictionary_freeze= True\n    X_name = wb.fit_transform(merge['name'])\n    del(wb)\n    X_name = X_name[:, np.array(np.clip(X_name.getnnz(axis=0) - 1, 0, 1), dtype=bool)]\n    print('[{}] Vectorize `name` completed.'.format(time.time() - start_time))\n\n    wb = CountVectorizer()\n    X_category1 = wb.fit_transform(merge['general_cat'])\n    X_category2 = wb.fit_transform(merge['subcat_1'])\n    X_category3 = wb.fit_transform(merge['subcat_2'])\n    print('[{}] Count vectorize `categories` completed.'.format(time.time() - start_time))\n\n    # wb= wordbatch.WordBatch(normalize_text, extractor=(WordBag, {\"hash_ngrams\": 3, \"hash_ngrams_weights\": [1.0, 1.0, 0.5],\n    wb = wordbatch.WordBatch(normalize_text, extractor=(WordBag, {\"hash_ngrams\": 2, \"hash_ngrams_weights\": [1.0, 1.0],\n                                                                  \"hash_size\": 2 ** 28, \"norm\": \"l2\", \"tf\": 1.0,\n                                                                  \"idf\": None})\n                             , procs=8)\n    wb.dictionary_freeze= True\n    X_description = wb.fit_transform(merge['item_description'])\n    del(wb)\n    X_description = X_description[:, np.array(np.clip(X_description.getnnz(axis=0) - 1, 0, 1), dtype=bool)]\n    print('[{}] Vectorize `item_description` completed.'.format(time.time() - start_time))\n\n    lb = LabelBinarizer(sparse_output=True)\n    X_brand = lb.fit_transform(merge['brand_name'])\n    print('[{}] Label binarize `brand_name` completed.'.format(time.time() - start_time))\n\n    X_dummies = csr_matrix(pd.get_dummies(merge[['item_condition_id', 'shipping']],\n                                          sparse=True).values)\n    print('[{}] Get dummies on `item_condition_id` and `shipping` completed.'.format(time.time() - start_time))\n    print(X_dummies.shape, X_description.shape, X_brand.shape, X_category1.shape, X_category2.shape, X_category3.shape,\n          X_name.shape)\n    sparse_merge = hstack((X_dummies, X_description, X_brand, X_category1, X_category2, X_category3, X_name)).tocsr()\n\n    print('[{}] Create sparse merge completed'.format(time.time() - start_time))\n\n    print(sparse_merge.shape)\n    \n \n    mask = np.array(np.clip(sparse_merge.getnnz(axis=0) - 1, 0, 1), dtype=bool)\n    sparse_merge = sparse_merge[:, mask]\n    X = sparse_merge[:nrow_train]\n    X_test = sparse_merge[nrow_test:]\n    print(sparse_merge.shape)\n\n    gc.collect()\n    train_X, train_y = X, y\n    if develop:\n        train_X, valid_X, train_y, valid_y = train_test_split(X, y, test_size=0.05, random_state=100)\n\n   \n    model = FTRL(alpha=0.01, beta=0.1, L1=0.00001, L2=1.0, D=sparse_merge.shape[1], iters=50, inv_link=\"identity\", threads=1)\n\n    model.fit(train_X, train_y)\n    print('[{}] Train FTRL completed'.format(time.time() - start_time))\n    if develop:\n        preds = model.predict(X=valid_X)\n        print(\"FTRL dev RMSLE:\", rmsle(np.expm1(valid_y), np.expm1(preds)))\n\n    predsF = model.predict(X_test)\n    print('[{}] Predict FTRL completed'.format(time.time() - start_time))\n \n    params = {\n        'learning_rate': 0.6,\n        'application': 'regression',\n        'max_depth': 4,\n        'num_leaves': 31,\n        'verbosity': -1,\n        'metric': 'RMSE',\n        'data_random_seed': 1,\n        'bagging_fraction': 0.6,\n        'bagging_freq': 5,\n        'feature_fraction': 0.6,\n        'nthread': 4,\n        'min_data_in_leaf': 100,\n        'max_bin': 31\n    }\n\n    # Remove features with document frequency <=100\n\n    print(sparse_merge.shape)\n    mask = np.array(np.clip(sparse_merge.getnnz(axis=0) - 100, 0, 1), dtype=bool)\n    sparse_merge = sparse_merge[:, mask]\n    X = sparse_merge[:nrow_train]\n    X_test = sparse_merge[nrow_test:]\n    print(sparse_merge.shape)\n\n    train_X, train_y = X, y\n    if develop:\n        train_X, valid_X, train_y, valid_y = train_test_split(X, y, test_size=0.05, random_state=100)\n    d_train = lgb.Dataset(train_X, label=train_y)\n    watchlist = [d_train]\n    if develop:\n        d_valid = lgb.Dataset(valid_X, label=valid_y)\n        watchlist = [d_train, d_valid]\n\n    model = lgb.train(params, train_set=d_train, num_boost_round=6000, valid_sets=watchlist, \\\n                      early_stopping_rounds=1000, verbose_eval=1000)\n\n    if develop:\n        preds = model.predict(valid_X)\n        print(\"LGB dev RMSLE:\", rmsle(np.expm1(valid_y), np.expm1(preds)))\n\n    predsL = model.predict(X_test)\n\n    print('[{}] Predict LGB completed.'.format(time.time() - start_time))\n\n\n    preds = predsF*0.3+predsL*0.7\n\n    submission['price'] = np.expm1(preds)\n    submission.to_csv(\"submission.csv\", index=False)\n\n\nif __name__ == '__main__':\n    main()"}],"metadata":{"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":4}