{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import Ridge\nfrom sklearn.pipeline import FeatureUnion\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Input, Dropout, Dense, concatenate, Embedding, Flatten, Activation\nfrom keras.optimizers import Adam\nfrom keras.models import Model\nfrom keras import backend as K","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!apt-get install p7zip\n!p7zip -d -f -k /kaggle/input/mercari-price-suggestion-challenge/train.tsv.7z\n!p7zip -d -f -k /kaggle/input/mercari-price-suggestion-challenge/test.tsv.7z","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def rmsle(Y, Y_pred):\n    # Y and Y_red have already been in log scale.\n    assert Y.shape == Y_pred.shape\n    return np.sqrt(np.mean(np.square(Y_pred - Y )))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df = pd.read_table(\"train.tsv\", sep=\"\\t\")\ntest_df = pd.read_csv(\"../input/mercari-price-suggestion-challenge/test_stg2.tsv.zip\" , sep='\\t')\n# test_df = pd.read_csv(\"test.tsv\", sep=\"\\t\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Handle missing data.\ndef fill_missing_values(df):\n    df.category_name.fillna(value=\"Other\", inplace=True)\n    df.brand_name.fillna(value=\"missing\", inplace=True)\n    df.item_description.fillna(value=\"None\", inplace=True)\n    return df\n\ntrain_df = fill_missing_values(train_df)\ntest_df = fill_missing_values(test_df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Scale target variable to log.\ntrain_df[\"target\"] = np.log1p(train_df.price)\n\n# Split training examples into train/dev examples.\ntrain_df, dev_df = train_test_split(train_df, random_state=347, train_size=0.70) \n\nY_train = train_df.target.values.reshape(-11, 1)\nY_dev = dev_df.target.values.reshape(-1, 1)\n\n# Calculate number of train/dev/test examples.\nn_trains = train_df.shape[0]\nn_devs = dev_df.shape[0]\nn_tests = test_df.shape[0]\nprint(\"Training on\", n_trains, \"examples\")\nprint(\"Validating on\", n_devs, \"examples\")\nprint(\"Testing on\", n_tests, \"examples\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Concatenate train - dev - test data for easy to handle\nfull_df = pd.concat([train_df, dev_df, test_df])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Processing categorical data...\")\nle = LabelEncoder()\n\nle.fit(full_df.category_name)\nfull_df.category_name = le.transform(full_df.category_name)\n\nle.fit(full_df.brand_name)\nfull_df.brand_name = le.transform(full_df.brand_name)\n\ndel le","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Transforming text data to sequences...\")\nraw_text = np.hstack([full_df.item_description.str.lower(), full_df.name.str.lower()])\n\nprint(\"   Fitting tokenizer...\")\ntok_raw = Tokenizer()\ntok_raw.fit_on_texts(raw_text)\n\nprint(\"   Transforming text to sequences...\")\nfull_df['seq_item_description'] = tok_raw.texts_to_sequences(full_df.item_description.str.lower())\nfull_df['seq_name'] = tok_raw.texts_to_sequences(full_df.name.str.lower())\n\ndel tok_raw","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"MAX_NAME_SEQ = 10\nMAX_ITEM_DESC_SEQ = 75\nMAX_TEXT = np.max([\n    np.max(full_df.seq_name.max()),\n    np.max(full_df.seq_item_description.max()),\n]) + 4\nMAX_CATEGORY = np.max(full_df.category_name.max()) + 1\nMAX_BRAND = np.max(full_df.brand_name.max()) + 1\nMAX_CONDITION = np.max(full_df.item_condition_id.max()) + 1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_keras_data(df):\n    X = {\n        'name': pad_sequences(df.seq_name, maxlen=MAX_NAME_SEQ),\n        'item_desc': pad_sequences(df.seq_item_description, maxlen=MAX_ITEM_DESC_SEQ),\n        'brand_name': np.array(df.brand_name),\n        'category_name': np.array(df.category_name),\n        'item_condition': np.array(df.item_condition_id),\n        'num_vars': np.array(df[[\"shipping\"]]),\n    }\n    return X\n\ntrain = full_df[:n_trains]\ndev = full_df[n_trains:n_trains+n_devs]\ntest = full_df[n_trains+n_devs:]\n\nX_train = get_keras_data(train)\nX_dev = get_keras_data(dev)\nX_test = get_keras_data(test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.layers import Conv1D,GlobalMaxPooling1D\ndef cnn_model(lr=0.001, decay=0.0):    \n    # Inputs\n    name = Input(shape=[X_train[\"name\"].shape[1]], name=\"name\")\n    item_desc = Input(shape=[X_train[\"item_desc\"].shape[1]], name=\"item_desc\")\n    brand_name = Input(shape=[1], name=\"brand_name\")\n    category_name = Input(shape=[1], name=\"category_name\")\n    item_condition = Input(shape=[1], name=\"item_condition\")\n    num_vars = Input(shape=[X_train[\"num_vars\"].shape[1]], name=\"num_vars\")\n\n    # Embeddings layers\n    emb_name = Embedding(MAX_TEXT, 20)(name)\n    emb_item_desc = Embedding(MAX_TEXT, 60)(item_desc)\n    emb_brand_name = Embedding(MAX_BRAND, 10)(brand_name)\n    emb_category_name = Embedding(MAX_CATEGORY, 10)(category_name)\n\n    convs1 = []\n    convs2 = []\n    \n    for filter_length in [1,2,3]:\n        cnn_layer1 = Conv1D(filters=50, kernel_size=filter_length, padding='same', activation='relu', strides=1) (emb_name)\n        cnn_layer2 = Conv1D(filters=50, kernel_size=filter_length, padding='same', activation='relu', strides=1) (emb_item_desc)\n        \n        maxpool1 = GlobalMaxPooling1D() (cnn_layer1)\n        maxpool2 = GlobalMaxPooling1D() (cnn_layer2)\n        \n        convs1.append(maxpool1)\n        convs2.append(maxpool2)\n\n    convs1 = concatenate(convs1)\n    convs2 = concatenate(convs2)\n    \n    # main layers\n    main_l = concatenate([\n        Flatten() (emb_brand_name),\n        Flatten() (emb_category_name),\n        item_condition,\n        convs1,\n        convs2,\n        num_vars,\n    ])\n\n    main_l = Dense(256)(main_l)\n    main_l = Activation('elu')(main_l)\n\n    main_l = Dense(128)(main_l)\n    main_l = Activation('elu')(main_l)\n\n    main_l = Dense(64)(main_l)\n    main_l = Activation('elu')(main_l)\n\n    # the output layer.\n    output = Dense(1, activation=\"linear\") (main_l)\n\n    model = Model([name, item_desc, brand_name , category_name, item_condition, num_vars], output)\n\n    optimizer = Adam(lr=lr, decay=decay)\n    model.compile(loss=\"mse\", optimizer=optimizer)\n\n    return model\n\nmodel = cnn_model()\nmodel.summary()\ndel model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"BATCH_SIZE = 1024\nepochs = 2\n\n# Calculate learning rate decay.\nexp_decay = lambda init, fin, steps: (init/fin)**(1/(steps-1)) - 1\nsteps = int(n_trains / BATCH_SIZE) * epochs\nlr_init, lr_fin = 0.007, 0.0005\nlr_decay = exp_decay(lr_init, lr_fin, steps)\n\nmodel = cnn_model(lr=lr_init, decay=lr_decay)\n\nprint(\"Fitting CNN model to training examples...\")\nmodel.fit(\n        X_train, Y_train, epochs=epochs, batch_size=BATCH_SIZE,\n        validation_data=(X_dev, Y_dev), verbose=2,\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Evaluating the model on validation data...\")\nY_dev_preds_rnn = model.predict(X_dev, batch_size=BATCH_SIZE)\nprint(\" RMSLE error:\", rmsle(Y_dev, Y_dev_preds_rnn))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rnn_preds = model.predict(X_test, batch_size=BATCH_SIZE, verbose=1)\nrnn_preds = np.expm1(rnn_preds)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission = test_df[[\"test_id\"]]\nsubmission[\"price\"] = rnn_preds","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission.to_csv(\"submission.csv\", index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}