{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!apt-get install p7zip\n!p7zip -d -f -k /kaggle/input/mercari-price-suggestion-challenge/train.tsv.7z\n!p7zip -d -f -k /kaggle/input/mercari-price-suggestion-challenge/test.tsv.7z\n!p7zip -d -f -k /kaggle/input/mercari-price-suggestion-challenge/sample_submission.csv.7z","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def rmsle(y, y_pred):\n    assert len(y) == len(y_pred)\n    to_sum = [(math.log(y_pred[i] + 1) - math.log(y[i] + 1)) ** 2.0 for i,pred in enumerate(y_pred)]\n    return (sum(to_sum) * (1.0/len(y))) ** 0.5","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.read_table(\"train.tsv\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def transform_category_name(category_name):\n    try:\n        main, sub1, sub2= category_name.split('/')\n        return main, sub1, sub2\n    except:\n        return \"none\", \"none\", \"none\"\n\ntrain['category_main'], train['subcat_1'], train['subcat_2'] = zip(*train['category_name'].apply(transform_category_name))\ntrain.head(50)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.isnull().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def handle_missing(dataset):\n    dataset.category_name.fillna(value=\"missing\", inplace=True)\n    dataset.brand_name.fillna(value=\"missing\", inplace=True)\n    dataset.item_description.fillna(value=\"missing\", inplace=True)\n    return (dataset)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = handle_missing(train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder, MinMaxScaler, StandardScaler\nprint(\"Handling categorical variables...\")\nle = LabelEncoder()\ntrain.category_main = le.fit_transform(train.category_main)\ntrain.subcat_1 = le.fit_transform(train.subcat_1)\ntrain.subcat_2 = le.fit_transform(train.subcat_2)\ntrain.brand_name = le.fit_transform(train.brand_name)\ndel le\n\ntrain.head(3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from string import punctuation\nfrom nltk import PorterStemmer\nfrom nltk.corpus import stopwords\npunctuation_symbols = []\nfor symbol in punctuation:\n    punctuation_symbols.append((symbol, ''))\n\n\ndef remove_punctuation(sentence: str) -> str:\n    return sentence.translate(str.maketrans('', '', string.punctuation))\n\n\ndef remove_digits(x):\n    x = ''.join([i for i in x if not i.isdigit()])\n    return x\n\n\nstop = stopwords.words('english')\n\n\ndef remove_stop_words(x):\n    x = ' '.join([i for i in x.lower().split(' ') if i not in stop])\n    return x\n\n\ndef to_lower(x):\n    return x.lower()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''import string\ntrain.item_description = train.item_description.astype(str)\ntrain['item_description'] = train['item_description'].apply(remove_digits)\ntrain['item_description'] = train['item_description'].apply(remove_punctuation)\ntrain['item_description'] = train['item_description'].apply(remove_stop_words)\ntrain['item_description'] = train['item_description'].apply(to_lower)\ntrain['name'] = train['name'].apply(remove_digits)\ntrain['name'] = train['name'].apply(remove_punctuation)\ntrain['name'] = train['name'].apply(remove_stop_words)\ntrain['name'] = train['name'].apply(to_lower)\ntrain.head(50) \n'''","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Text to seq process...\")\nfrom keras.preprocessing.text import Tokenizer\nraw_text = np.hstack([train.item_description.str.lower(), train.name.str.lower()])\n\nprint(\"   Fitting tokenizer...\")\ntok_raw = Tokenizer(num_words=3000)\ntok_raw.fit_on_texts(raw_text)\nprint(\"   Transforming text to seq...\")\n\ntrain[\"seq_item_description\"] = tok_raw.texts_to_sequences(train.item_description.str.lower())\ntrain[\"seq_name\"] = tok_raw.texts_to_sequences(train.name.str.lower())\ntrain.head(3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"max_name_seq = np.max(train.seq_name.apply(lambda x: len(x)))\nmax_seq_item_description = np.max(train.seq_item_description.apply(lambda x: len(x)))\nprint(\"max name seq \"+str(max_name_seq))\nprint(\"max item desc seq \"+str(max_seq_item_description))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"MAX_NAME_SEQ = 10\nMAX_ITEM_DESC_SEQ = 75\nMAX_TEXT = np.max([np.max(train.seq_name.max()),np.max(train.seq_item_description.max())])+2\nMAX_CATEGORY =np.max([np.max(train.category_main.max()),np.max(train.subcat_1.max()), np.max(train.subcat_2.max())])+2\nMAX_BRAND = train.brand_name.max()+1\nMAX_CONDITION = train.item_condition_id.max()+1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train[\"target\"] = np.log(train.price+1)\ntarget_scaler = MinMaxScaler(feature_range=(-1, 1))\ntrain[\"target\"] = target_scaler.fit_transform(train.target.values.reshape(-1,1))\npd.DataFrame(train.target).hist()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\ndtrain, dvalid = train_test_split(train, random_state=123, train_size=0.70)\nprint(dtrain.shape)\nprint(dvalid.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.preprocessing.sequence import pad_sequences\n\ndef get_keras_data(dataset):\n    X = {\n        'name': pad_sequences(dataset.seq_name, maxlen=MAX_NAME_SEQ)\n        ,'item_desc': pad_sequences(dataset.seq_item_description, maxlen=MAX_ITEM_DESC_SEQ)\n        ,'brand_name': np.array(dataset.brand_name)\n        ,'category_main': np.array(dataset.category_main)\n        ,'subcat_1': np.array(dataset.subcat_1)\n        ,'subcat_2': np.array(dataset.subcat_2)\n        ,'item_condition': np.array(dataset.item_condition_id)\n        ,'num_vars': np.array(dataset[[\"shipping\"]])\n    }\n    return X\n\nX_train = get_keras_data(dtrain)\nX_valid = get_keras_data(dvalid)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.layers import Input, Dropout, Dense, BatchNormalization, Activation, concatenate, GRU, Embedding, Flatten, BatchNormalization\nfrom keras.models import Model\nfrom keras.callbacks import ModelCheckpoint, Callback, EarlyStopping\nfrom keras import backend as K\n\ndef get_callbacks(filepath, patience=2):\n    es = EarlyStopping('val_loss', patience=patience, mode=\"min\")\n    msave = ModelCheckpoint(filepath, save_best_only=True)\n    return [es, msave]\n\ndef rmsle_cust(y_true, y_pred):\n    first_log = K.log(K.clip(y_pred, K.epsilon(), None) + 1.)\n    second_log = K.log(K.clip(y_true, K.epsilon(), None) + 1.)\n    return K.sqrt(K.mean(K.square(first_log - second_log), axis=-1))\n\ndef get_model():\n    \n    #Inputs\n    name = Input(shape=[X_train[\"name\"].shape[1]], name=\"name\")\n    item_desc = Input(shape=[X_train[\"item_desc\"].shape[1]], name=\"item_desc\")\n    brand_name = Input(shape=[1], name=\"brand_name\")\n    category_main = Input(shape=[1], name=\"category_main\")\n    subcat_1 = Input(shape=[1], name=\"subcat_1\")\n    subcat_2 = Input(shape=[1], name=\"subcat_2\")\n    item_condition = Input(shape=[1], name=\"item_condition\")\n    num_vars = Input(shape=[X_train[\"num_vars\"].shape[1]], name=\"num_vars\")\n    \n    \n    #Embeddings layers\n    emb_name = Embedding(MAX_TEXT, 50)(name)  #embedding comes from word2vector, I is only used in initial layer and it purpose is to recognize possible similarities in the mapped (here 50-dim, 10-dim and 5-dim space)\n    emb_item_desc = Embedding(MAX_TEXT, 50)(item_desc)\n    emb_brand_name = Embedding(MAX_BRAND, 10)(brand_name)\n    emb_category_main = Embedding(MAX_CATEGORY, 10)(category_main)\n    emb_subcat_1 = Embedding(MAX_CATEGORY, 10)(subcat_1)\n    emb_subcat_2 = Embedding(MAX_CATEGORY, 10)(subcat_2)\n    emb_item_condition = Embedding(MAX_CONDITION, 5)(item_condition)\n    #rnn layer\n    \n    rnn_layer1 = GRU(16) (emb_item_desc)  #GRU, part of the recurennt NN, notice that we applied it only on the textual data that we transofrem into sequnce ,i.e. numerical\n    rnn_layer2 = GRU(8) (emb_name)  # 16,8 and stands for the dimensionality of the output space, i.e. what we are going to \"give to the next layer\"\n    \n    #main layer. Note its a keras concatenate, meaning it will merge layers of neural network\n    #Role of flatten in keras: Let us say that  \"emb_brand_name\" has elements of dimension 3x2 for example. To make it 1-d we use flatten\n    # Further layers may need 1-d vectors as input\n    \n    main_l = concatenate([\n        Flatten() (emb_brand_name)\n        , Flatten() (emb_category_main)\n        , Flatten() (emb_subcat_1)\n        , Flatten() (emb_subcat_2)\n        , Flatten() (emb_item_condition)\n        , rnn_layer1\n        , rnn_layer2\n        , num_vars\n    ])\n    main_l = Dropout(0.25) (Dense(128) (main_l))\n    main_l = Dropout(0.1) (Dense(64) (main_l))\n    \n    #output (1 stands for one output neuron and it should tell us the cost of the item)\n    output = Dense(1, activation=\"linear\") (main_l)\n    \n    #model\n    model = Model([name, item_desc, brand_name\n                   , category_main, subcat_1, subcat_2, item_condition, num_vars], output)\n    model.compile(loss='mse', optimizer='adam')\n    \n    return model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.layers import Flatten, Input, Embedding, Dense, Dropout, Conv1D, GlobalMaxPooling1D\nfrom keras.layers.merge import concatenate\nfrom keras.models import Model\nfrom keras.regularizers import Regularizer\nfrom keras.callbacks import TensorBoard, ModelCheckpoint\n\ndef get_model():  \n    \n    #Inputs\n    name = Input(shape=[X_train[\"name\"].shape[1]], name=\"name\")\n    item_desc = Input(shape=[X_train[\"item_desc\"].shape[1]], name=\"item_desc\")\n    brand_name = Input(shape=[1], name=\"brand_name\")\n    category_main = Input(shape=[1], name=\"category_main\")\n    subcat_1 = Input(shape=[1], name=\"subcat_1\")\n    subcat_2 = Input(shape=[1], name=\"subcat_2\")\n    item_condition = Input(shape=[1], name=\"item_condition\")\n    num_vars = Input(shape=[X_train[\"num_vars\"].shape[1]], name=\"num_vars\")\n    \n    #Embeddings layers\n    emb_name = Embedding(MAX_TEXT, 50)(name)  #embedding comes from word2vector, I is only used in initial layer and it purpose is to recognize possible similarities in the mapped (here 50-dim, 10-dim and 5-dim space)\n    emb_item_desc = Embedding(MAX_TEXT, 50)(item_desc)\n    emb_brand_name = Embedding(MAX_BRAND, 10)(brand_name)\n    emb_category_main = Embedding(MAX_CATEGORY, 10)(category_main)\n    emb_subcat_1 = Embedding(MAX_CATEGORY, 10)(subcat_1)\n    emb_subcat_2 = Embedding(MAX_CATEGORY, 10)(subcat_2)\n    emb_item_condition = Embedding(MAX_CONDITION, 5)(item_condition)\n    \n    convs1 = []\n    convs2 = []\n    \n    for filter_length in [1,2]:\n        cnn_layer1 = Conv1D(filters=50, kernel_size=filter_length, padding='same', activation='relu', strides=1) (emb_name)\n        cnn_layer2 = Conv1D(filters=50, kernel_size=filter_length, padding='same', activation='relu', strides=1) (emb_item_desc)\n        \n        maxpool1 = GlobalMaxPooling1D() (cnn_layer1)\n        maxpool2 = GlobalMaxPooling1D() (cnn_layer2)\n        \n        convs1.append(maxpool1)\n        convs2.append(maxpool2)\n\n    convs1 = concatenate(convs1)\n    convs2 = concatenate(convs2)\n    \n    main_l = concatenate([\n            Flatten() (emb_category_main),\n            Flatten() (emb_subcat_1),\n            Flatten() (emb_subcat_2),\n            Flatten() (emb_brand_name),\n            Flatten() (emb_item_condition),\n            convs1, \n            convs2, \n            num_vars\n    ])\n    main_l = Dropout(0.25)(Dense(128, activation='relu') (main_l)) #.25 = .435\n    main_l = Dropout(0.1)(Dense(64, activation='relu') (main_l)) #.1\n    \n    # , kernel_regularizer=keras.regularizers.l2(0.01)\n    output = Dense(1, activation='linear') (main_l)\n\n    model = Model([name, item_desc, brand_name\n                   , category_main, subcat_1, subcat_2, item_condition, num_vars], output)\n    model.compile(loss='mse', optimizer='adam')\n    \n    return model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"BATCH_SIZE = 20000\nepochs = 5\n\nmodel = get_model()\nmodel.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.fit(X_train, dtrain.target, epochs=epochs, batch_size=BATCH_SIZE\n          , validation_data=(X_valid, dvalid.target)\n          , verbose=1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import math\n\nval_preds = model.predict(X_valid)\nval_preds = target_scaler.inverse_transform(val_preds)\nval_preds = np.exp(val_preds)+1\n\n#mean_absolute_error, mean_squared_log_error\ny_true = np.array(dvalid.price.values)\ny_pred = val_preds[:,0]\nv_rmsle = rmsle(y_true, y_pred)\nprint(\" RMSLE error on dev test: \"+str(v_rmsle))","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}